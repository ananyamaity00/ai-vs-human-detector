# detector.py
import re
from collections import Counter
import math

# small set of stopwords (no external dependency)
_STOPWORDS = {
    "the","and","is","in","it","of","to","a","that","i","you","for","on","with","as","this",
    "are","was","but","be","they","have","not","or","at","from","by","an"
}

_AI_DISCLAIMERS = [
    "as an ai language model", "i'm an ai", "i am an ai", "as a language model",
    "i don't have personal", "i do not have personal", "i cannot browse the internet",
    "i do not have personal opinions", "generated by ai"
]

def _top_ngram_repeat_ratio(words, n=3):
    if len(words) < n:
        return 0.0
    ngrams = [' '.join(words[i:i+n]).lower() for i in range(len(words)-n+1)]
    counts = Counter(ngrams)
    most_common_count = counts.most_common(1)[0][1]
    return most_common_count / max(1, len(ngrams))

def extract_features(text):
    text_stripped = text.strip()
    words = re.findall(r"\w+", text_stripped)
    n_words = len(words)
    avg_word_len = (sum(len(w) for w in words) / n_words) if n_words else 0.0

    sentences = re.split(r'[.!?]+', text_stripped)
    sentences = [s.strip() for s in sentences if s.strip()]
    avg_sent_len = (sum(len(s.split()) for s in sentences) / len(sentences)) if sentences else 0.0

    punctuation_count = sum(1 for c in text_stripped if c in '.,;:!?()[]{}"\'')
    punct_ratio = punctuation_count / (len(text_stripped) + 1)

    unique_ratio = len(set(w.lower() for w in words)) / (n_words + (0 if n_words else 1))

    stopword_count = sum(1 for w in words if w.lower() in _STOPWORDS)
    stopword_ratio = (stopword_count / n_words) if n_words else 0.0

    trigram_repeat = _top_ngram_repeat_ratio(words, n=3)

    contains_ai_disclaimer = any(d in text_stripped.lower() for d in _AI_DISCLAIMERS)

    return {
        "n_words": n_words,
        "avg_word_len": round(avg_word_len, 3),
        "avg_sent_len": round(avg_sent_len, 3),
        "punct_ratio": round(punct_ratio, 5),
        "unique_ratio": round(unique_ratio, 3),
        "stopword_ratio": round(stopword_ratio, 3),
        "trigram_repeat": round(trigram_repeat, 4),
        "contains_ai_disclaimer": contains_ai_disclaimer
    }

def detect_ai_content(text: str) -> dict:
    """
    Heuristic AI vs Human detector.
    Returns a dict: { prediction, confidence, features, note }
    - For very short texts (<20 words) returns a special human-like message.
    """
    text = (text or "").strip()
    features = extract_features(text)
    n_words = features["n_words"]

    # Handle empty input
    if n_words == 0:
        return {
            "prediction": "Empty Text ‚ö†Ô∏è",
            "confidence": 0.0,
            "features": features,
            "note": "No text provided."
        }

    # If text is very short, don't try to over-interpret ‚Äî assume human and give note
    if n_words < 20:
        return {
            "prediction": "Likely Human-written üßë‚Äçüíª (text too short for AI patterns)",
            "confidence": 0.20,
            "features": features,
            "note": "Short inputs (<20 words) are ambiguous; we classify as human by default."
        }

    # indicators based on features (tunable)
    indicators = {}
    indicators['disclaimer'] = 1.0 if features["contains_ai_disclaimer"] else 0.0
    indicators['long_sent'] = 1.0 if features["avg_sent_len"] >= 20 else 0.0
    indicators['long_words'] = 1.0 if features["avg_word_len"] >= 4.6 else 0.0
    indicators['low_punct'] = 1.0 if features["punct_ratio"] < 0.03 else 0.0
    indicators['high_vocab_diversity'] = 1.0 if features["unique_ratio"] >= 0.65 else 0.0
    indicators['low_trigram_repeat'] = 1.0 if features['trigram_repeat'] < 0.02 else 0.0
    indicators['low_stopword'] = 1.0 if features['stopword_ratio'] < 0.25 else 0.0

    # weights (you can tune these later)
    weights = {
        'disclaimer': 3.0,
        'long_sent': 1.0,
        'long_words': 0.9,
        'low_punct': 0.8,
        'high_vocab_diversity': 0.8,
        'low_trigram_repeat': 0.5,
        'low_stopword': 0.4
    }

    weighted_sum = sum(indicators[k] * weights.get(k, 1.0) for k in indicators)
    max_possible = sum(weights.values())
    # confidence normalized to [0,1]
    confidence = weighted_sum / max_possible if max_possible > 0 else 0.0
    confidence = max(0.0, min(1.0, confidence))

    # label threshold
    label = "Likely AI-generated ü§ñ" if confidence >= 0.45 else "Likely Human-written üßë‚Äçüíª"

    return {
        "prediction": label,
        "confidence": round(confidence, 3),
        "features": features,
        "indicators": indicators,
        "weights": weights,
        "weighted_sum": round(weighted_sum, 3),
        "note": "Heuristic detector: good for demos. For higher accuracy, train an ML model with labeled data."
    }


# quick local test if file run directly
if __name__ == "__main__":
    sample = "As an AI language model, I don't have personal opinions, but I can provide a thorough explanation."
    result = detect_ai_content(text)

