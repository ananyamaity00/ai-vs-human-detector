# detector_v2.py
import re
from collections import Counter
import math
import statistics

# small set of stopwords (no external dependency)
_STOPWORDS = {
    "the","and","is","in","it","of","to","a","that","i","you","for","on","with","as","this",
    "are","was","but","be","they","have","not","or","at","from","by","an"
}

_AI_DISCLAIMERS = [
    "as an ai language model", "i'm an ai", "i am an ai", "as a language model",
    "i don't have personal", "i do not have personal", "i cannot browse the internet",
    "i do not have personal opinions", "generated by ai", "generated with"
]

_TEMPLATE_PHRASES = [
    "in conclusion", "to summarize", "in summary", "firstly", "secondly",
    "in this article", "in this post", "as a result", "to conclude", "furthermore",
    "moreover", "on the other hand"
]

# emoji regex (common ranges)
_EMOJI_RE = re.compile(
    "[" 
    "\U0001F600-\U0001F64F"  # emoticons
    "\U0001F300-\U0001F5FF"  # symbols & pictographs
    "\U0001F680-\U0001F6FF"  # transport & map symbols
    "\U0001F1E0-\U0001F1FF"  # flags
    "]", flags=re.UNICODE
)

def _top_ngram_repeat_ratio(words, n=3):
    if len(words) < n:
        return 0.0
    ngrams = [' '.join(words[i:i+n]).lower() for i in range(len(words)-n+1)]
    counts = Counter(ngrams)
    most_common_count = counts.most_common(1)[0][1]
    return most_common_count / max(1, len(ngrams))

def extract_features(text: str) -> dict:
    text_stripped = text.strip()
    # word tokens (basic)
    words = re.findall(r"\w+", text_stripped)
    n_words = len(words)
    avg_word_len = (sum(len(w) for w in words) / n_words) if n_words else 0.0

    # sentences and sentence lengths
    sentences = re.split(r'(?<=[.!?])\s+', text_stripped)
    sentences = [s.strip() for s in sentences if s.strip()]
    sent_lens = [len(s.split()) for s in sentences] if sentences else []
    avg_sent_len = (sum(sent_lens) / len(sent_lens)) if sent_lens else 0.0
    burstiness = (statistics.pstdev(sent_lens) / avg_sent_len) if (sent_lens and avg_sent_len > 0) else 0.0

    punctuation_count = sum(1 for c in text_stripped if c in '.,;:!?()[]{}"\'')
    punct_ratio = punctuation_count / (len(text_stripped) + 1)

    unique_ratio = len(set(w.lower() for w in words)) / (n_words + 1e-9)

    stopword_count = sum(1 for w in words if w.lower() in _STOPWORDS)
    stopword_ratio = (stopword_count / n_words) if n_words else 0.0

    trigram_repeat = _top_ngram_repeat_ratio(words, n=3)

    contains_ai_disclaimer = any(d in text_stripped.lower() for d in _AI_DISCLAIMERS)

    # markdown / bullets / emojis / templates
    markdown_hits = text_stripped.count('```') + text_stripped.count('**') + text_stripped.count('# ')
    lines = [ln for ln in text_stripped.splitlines() if ln.strip()]
    bullet_count = sum(1 for ln in lines if re.match(r'^\s*([-*+‚Ä¢])\s+', ln))
    bullet_density = (bullet_count / max(1, len(lines))) if lines else 0.0

    emoji_count = len(_EMOJI_RE.findall(text_stripped))
    emoji_density = emoji_count / (n_words + 1e-9)

    template_hits = sum(text_stripped.lower().count(p) for p in _TEMPLATE_PHRASES)
    # include AI disclaimers as part of template hits
    template_hits = template_hits + (1 if contains_ai_disclaimer else 0)

    return {
        "n_words": n_words,
        "avg_word_len": round(avg_word_len, 3),
        "avg_sent_len": round(avg_sent_len, 3),
        "burstiness": round(burstiness, 4),
        "punct_ratio": round(punct_ratio, 5),
        "markdown_hits": markdown_hits,
        "bullet_density": round(bullet_density, 4),
        "emoji_density": round(emoji_density, 4),
        "template_hits": template_hits,
        "trigram_repetition": round(trigram_repeat, 4),
        "unique_word_ratio": round(unique_ratio, 3),
        "stopword_ratio": round(stopword_ratio, 3),
        "contains_ai_disclaimer": contains_ai_disclaimer
    }

def analyze_text(text: str) -> dict:
    """
    Heuristic analyzer that returns:
      { prediction, confidence, features, contributions, top_contributors, note }
    Designed to be plugged into the Streamlit UI (app.py).
    """
    text = (text or "").strip()
    features = extract_features(text)
    n_words = features["n_words"]

    # Empty input handling
    if n_words == 0:
        return {
            "prediction": "Empty Text ‚ö†Ô∏è",
            "confidence": 0.0,
            "features": features,
            "contributions": {},
            "top_contributors": [],
            "note": "No text provided."
        }

    # Short text heuristic
    if n_words < 20:
        return {
            "prediction": "Likely Human-written üßë‚Äçüíª (text too short for AI patterns)",
            "confidence": 0.20,
            "features": features,
            "contributions": {},
            "top_contributors": [],
            "note": "Short inputs (<20 words) are ambiguous; we classify as human by default."
        }

    # indicators
    indicators = {}
    indicators['disclaimer'] = 1.0 if features.get("contains_ai_disclaimer") else 0.0
    indicators['long_sent'] = 1.0 if features.get("avg_sent_len", 0.0) >= 20 else 0.0
    indicators['long_words'] = 1.0 if features.get("avg_word_len", 0.0) >= 4.6 else 0.0
    indicators['low_punct'] = 1.0 if features.get("punct_ratio", 0.0) < 0.03 else 0.0
    indicators['high_vocab_diversity'] = 1.0 if features.get("unique_word_ratio", 0.0) >= 0.65 else 0.0
    indicators['low_trigram_repeat'] = 1.0 if features.get("trigram_repetition", 0.0) < 0.02 else 0.0
    indicators['low_stopword'] = 1.0 if features.get("stopword_ratio", 0.0) < 0.25 else 0.0

    # weights (tunable)
    weights = {
        'disclaimer': 3.0,
        'long_sent': 1.0,
        'long_words': 0.9,
        'low_punct': 0.8,
        'high_vocab_diversity': 0.8,
        'low_trigram_repeat': 0.5,
        'low_stopword': 0.4
    }

    weighted_sum = sum(indicators[k] * weights.get(k, 1.0) for k in indicators)
    max_possible = sum(weights.values())
    confidence = weighted_sum / max_possible if max_possible > 0 else 0.0
    confidence = max(0.0, min(1.0, confidence))

    # label threshold
    label = "Likely AI-generated ü§ñ" if confidence >= 0.45 else "Likely Human-written üßë‚Äçüíª"

    # Contributions (as percentage points)
    label_map = {
        'disclaimer': 'AI disclaimer present',
        'long_sent': 'Long sentences',
        'long_words': 'Longer avg word length',
        'low_punct': 'Low punctuation density',
        'high_vocab_diversity': 'High vocabulary diversity',
        'low_trigram_repeat': 'Low trigram repetition',
        'low_stopword': 'Low stopword ratio'
    }

    contributions = {}
    for k, v in indicators.items():
        pct = (v * weights.get(k, 1.0) / max_possible) * 100.0  # percent points
        contributions[label_map.get(k, k)] = round(pct, 2)

    # sort top contributors
    sorted_items = sorted(contributions.items(), key=lambda x: x[1], reverse=True)
    top_contributors = [(k, v) for k, v in sorted_items if v > 0][:3]

    # prepare features returned to UI (add ai_score for app)
    features_out = dict(features)
    features_out["ai_score"] = round(confidence, 3)

    return {
        "prediction": label,
        "confidence": round(confidence, 3),
        "features": features_out,
        "contributions": contributions,
        "top_contributors": top_contributors,
        "indicators": indicators,
        "weights": weights,
        "weighted_sum": round(weighted_sum, 3),
        "note": "Heuristic detector: good for demos. For higher accuracy, train an ML model with labeled data."
    }

# quick local test
if __name__ == "__main__":
    sample = "As an AI language model, I don't have personal opinions, but I can provide a thorough explanation."
    result = analyze_text(sample)
    import json
    print(json.dumps(result, indent=2))
